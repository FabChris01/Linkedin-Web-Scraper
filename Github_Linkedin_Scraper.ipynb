{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re as re\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Webdriver path: D:\\\\Projects\\\\Intern\\\\Machine_Learning\\\\tag\\\\Tag_Generation\\\\chromedriver.exe\n",
      "Enter the username: fabchris2001@gmail.com\n",
      "Enter the password: IaFC2001@L!\n",
      "D:\\\\Projects\\\\Intern\\\\Machine_Learning\\\\tag\\\\Tag_Generation\\\\chromedriver.exe\n",
      "fabchris2001@gmail.com\n",
      "IaFC2001@L!\n"
     ]
    }
   ],
   "source": [
    "# Get Webdriver path, username and password\n",
    "PATH = input(\"Enter the Webdriver path: \")\n",
    "USERNAME = input(\"Enter the username: \")\n",
    "PASSWORD = input(\"Enter the password: \")\n",
    "print(PATH)\n",
    "print(USERNAME)\n",
    "print(PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use driver to open the link\n",
    "driver = webdriver.Chrome(PATH)\n",
    "driver.get(\"https://www.linkedin.com/uas/login\")\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use login credentials to login\n",
    "email=driver.find_element_by_id(\"username\")\n",
    "email.send_keys(USERNAME)\n",
    "password=driver.find_element_by_id(\"password\")\n",
    "password.send_keys(PASSWORD)\n",
    "time.sleep(3)\n",
    "password.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lists\n",
    "post_links = []\n",
    "post_texts = []\n",
    "post_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our web scraper Function\n",
    "def Scrape_func(a,b,c):\n",
    "    name = a[28:-1]\n",
    "    page = a\n",
    "    time.sleep(10)\n",
    "\n",
    "    driver.get(page + 'detail/recent-activity/shares/')  \n",
    "    start=time.time()\n",
    "    lastHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        newHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if newHeight == lastHeight:\n",
    "            break\n",
    "        lastHeight = newHeight\n",
    "        end=time.time()\n",
    "        if round(end-start)>20:\n",
    "            break\n",
    "\n",
    "    company_page = driver.page_source   \n",
    "\n",
    "    linkedin_soup = bs(company_page.encode(\"utf-8\"), \"html\")\n",
    "    linkedin_soup.prettify()\n",
    "    containers = linkedin_soup.findAll(\"div\",{\"class\":\"occludable-update ember-view\"})\n",
    "    print(\"Fetching data from account: \"+ name)\n",
    "    iterations = 0\n",
    "    nos = int(input(\"Enter number of posts: \"))\n",
    "    for container in containers:\n",
    "\n",
    "        try:\n",
    "            text_box = container.find(\"div\",{\"class\":\"feed-shared-update-v2__description-wrapper ember-view\"})\n",
    "            text = text_box.find(\"span\",{\"dir\":\"ltr\"})\n",
    "            b.append(text.text.strip())\n",
    "            c.append(name)\n",
    "            iterations += 1\n",
    "            print(iterations)\n",
    "            \n",
    "            if(iterations==nos):\n",
    "                break\n",
    "\n",
    "        except:\n",
    "            pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of entries: 2\n",
      "Enter the link: https://www.linkedin.com/in/williamhgates/\n",
      "Enter the link: https://www.linkedin.com/in/philipvollet/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabch\\anaconda3\\envs\\tag\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\fabch\\anaconda3\\envs\\tag\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from account: williamhgates\n",
      "Enter number of posts: 7\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Fetching data from account: philipvollet\n",
      "Enter number of posts: 3\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Using recursion with our function\n",
    "n = int(input(\"Enter the number of entries: \"))\n",
    "for i in range(n):\n",
    "    post_links.append(input(\"Enter the link: \"))\n",
    "for j in range(n):\n",
    "    Scrape_func(post_links[j],post_texts,post_names)\n",
    "\n",
    "        \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data\n",
    "data = {\n",
    "    \"Name\": post_names,\n",
    "    \"Content\": post_texts,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"test1.csv\", encoding='utf-8', index=False)\n",
    "\n",
    "writer = pd.ExcelWriter(\"test1.xlsx\", engine='xlsxwriter')\n",
    "df.to_excel(writer, index =False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
